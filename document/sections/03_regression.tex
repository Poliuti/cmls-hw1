\section{Regression}\label{sec:regression}

Once features have been extracted, regression theory allows to predict a real value from a set of $M$ given inputs, where $M$ is the dimension of the features space.
In this context, dimensional \textbf{Music Emotion Recognition} has been considered as a regression problem where distinct regressors are trained \textit{independently} for valence and arousal. 

In particular, four regressors

\[
	r \colon \R^{M} \to \R
\]

will be trained to predict the four-element vector

\begin{center}
	[valence mean, valence std, arousal mean, arousal std]
\end{center}

Different regression models are used to find out which set of regressors best fit the data. In particular we focused on three families of regressors: \textbf{Linear Regressors}, \textbf{Support Vector Machines} and \textbf{K-Neighbors Regressors}. Performance evaluation will be achieved by means of the metrics represented by $R^2$ and $MSE$ statistics (see section~\ref{sec:metrics}).

Regressors are implemented by taking advantage of the Python library \textit{Scikit-learn}, which integrates a wide range of state-of-the-art machine learning algorithms \cite{scikit-learn}.
Regression training is not computed until the dataset is consistently rearranged and split. In fact, in order to split our collection of features and annotations in the two parts of training and testing set it will be necessary to shuffle our initial dataset so that it will be as inhomogeneous as possible in terms of music genre. \todo{ripetizione di evaluation?}
Although we will focus on Music Emotion Recognition for the global layer of the song, the regression approach is capable to fit also to music emotion variation detection (\textbf{MEVD}), considering the time evolution of features frame-by-frame for each song.

Linear regression is a linear model in which coefficients are computed to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation \cite{scikit-learn}.
It represented our first attempt because of the efficiency in computational terms and consequently had the role to facilitate the search for the best features. Nevertheless another type of algorithm was chosen for the best regressor. \todo[inline]{menzionare anche stochastic e ridge?}

\textbf{Support Vector Regression} is our second attempt and it turned out to be the best approach.
\textbf{Support Vector Machines} represent the operation to map our features space to a higher dimensional one and learn a nonlinear function by a linear learning machine in the kernel-induced feature space, where data are more separable \cite{yang2008regression}.
The \textbf{SVR} algorithm supplied by \textit{Scikit-learn} provides an easy way to compute regressors based on different kernels.
Out of some attempts involving linear, polynomial, radial basis function (RBF) and sigmoid kernels we found that each of them behaves much better than Linear and K-Neighbors regressors. \todo[inline]{ricontrollare kernel (usiamo solo rbf e sigmoid), menzionare $\epsilon$ e $\nu$ e free paramenters}

The latter belongs to the class of unsupervised and supervised neighbors-based learning methods.
The workflow of the algorithm is to detect the closest points in distance to the new one and extrapolate from them the value of the label.
The $k$ number of neighbors is predefined and it is up to the user. \todo{noi usiamo la cross per trovare $k$}
The basic implementation of the algorithm uses uniform weights for each neighbor meaning that each sample contribution is the same.
We found advantageous to assign different weights to the samples depending on the inverse of the distance from the new point.
The \texttt{KNeighborsRegressor} algorithm supplied by \textit{Scikit-learn} easily provides the keyword \texttt{weights = `distance'} to achieve this goal. \todo[inline]{in realtà con la cross quel che si comporta meglio è uniform}

Cross-validation is used to find the best parameters for each regressor and the procedure will be described in detail in the next section.
