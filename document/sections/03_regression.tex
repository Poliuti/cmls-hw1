\section{Regression models}\label{sec:regression}

Once features have been extracted, regression theory allows to predict a real value from a set of $M$ given inputs, where $M$ is the dimension of the features space.
In this context, dimensional \textbf{Music Emotion Recognition} has been considered as a regression problem where distinct regressors are trained \textit{independently} for valence and arousal.
Although our focus was on Music Emotion Recognition for each song as a whole, the regression approach is also suitable for music emotion variation detection (\textbf{MEVD}), considering the frame-by-frame time evolution of features for each song.

In particular, four regressors
\[
	r \colon \R^{M} \to \R
\]
will be trained to predict the four-element vector

\begin{center}
	[valence mean, valence std, arousal mean, arousal std]
\end{center}

Different regression models are used to find out which set of regressors best fit the data. In particular we focused on three families of regressors: \emph{Linear Regressors}, \emph{Support Vector Machines} and \emph{K-Neighbors Regressors}.
Performance evaluation will be achieved by means of the metrics represented by $R^2$ and MSE statistics (see section~\ref{sec:metrics}).
For our experiments, we employed the Python library \textit{Scikit-learn}, which integrates a wide range of state-of-the-art machine learning algorithms \cite{scikit-learn}.

\paragraph{Linear models}
In the simplest linear model, coefficients are computed to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.
Other linear models allow to tune different open variables in order to better fit to the data, in particular we tried the \emph{Ridge} regressor with built-in cross-validation and the \emph{Stochastic Gradient Descent} regressor.
The latter is set to be $\epsilon$-insensitive, which is the same loss function used in SVR.
The open variables of SGD are the threshold $\epsilon$ and the $\alpha$ constant that multiplies the regularization term, while Ridge has only $\alpha$ as open variable. The higher the value of $\alpha$, the stronger the regularization. In particular, \texttt{RidgeCV} performs a cross-validation using, by default, values for $\alpha = 0.1, 1, 10$ (see section~\ref{sec:cross-validation} for a complete description on cross-validation).
It represented our first attempt because of the efficiency in computational terms and consequently had a role in facilitating the feature selection.

\paragraph{Support Vector Machines}
The idea behind SVM is to map the feature space to a higher dimension space, in order to be able to learn a nonlinear function by a linear learning machine in the kernel-induced feature space, where data are more separable \cite{yang2008regression}.
Sklearn provides two types of support vector regressors, i.e. \texttt{NuSVR} and \texttt{SVR}. The former is an implementation of a $\nu$-based SVM, while the latter implements an $\epsilon$-based SVM. Both algorithms also have a penalty parameter $C$, which is inversely proportional to $\alpha$ of the previously mentioned linear models.
Another free parameter in the SVM model is the kernel function. In our experiments we tried radial basis function (RBF) kernels and sigmoid kernels, as they were the ones requiring less computational power.

\paragraph{K-neighboors}
For variety, we also attempted using neighbors-based learning methods.
The idea behind \emph{K-Neighbors} models is to detect the closest sample points in distance to the considered sample point, and predict from them the target value. The main open parameter is thus $k$, which is the number of neighboors points to consider.
Each sample's contribution can also be weighted. In particular, the sklearn implementation provides a ``uniform'' or a ``distance'' weighting. In the first mode each point has the same weight, whereas in the second one each point is weighted in an inversely proportional manner in respect to its distance to the considered sample point.

\vspace{1em}

Cross-validation is used to find the best parameters for each regressor and the procedure will be described in detail in the next section.
