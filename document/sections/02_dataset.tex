\section{Dataset}
In our work we employ the DEAM Dataset (\textit{MediaEval Database for Emotional Analysis in Music}) provided by the \textit{Swiss Center for Affective Sciences of the University of Geneva, Switzerland}.

The dataset consists of 2058 songs annotated with valence and arousal values both continuously (per-second) and over the whole song.
 The dataset from 2013 and 2014 contains annotations on 45 seconds excerpts extracted from random points in songs. Dataset from 2015 has annotations on full songs. Both the 45 seconds clips and full songs are provided in MPEG layer 3 (MP3) format.
Averaged annotations have been generated with 2Hz sampling rate and in addition to the average, the standard deviation of the annotations is provided.~\cite{soleymani2016deam}

In particular, our work is focused on the first 2000 songs (45 seconds excerpts),  on the averaged annotations of the dataset and on the features extracted from the music database, both features already given with the dataset and features extracted ex novo.

\subsection{Music database}

The music database consists of royalty-free music from several sources: \textit{freemusicarchive.org} (FMA), \textit{jamendo.com}, and the \textit{medleyDB dataset} \cite{bittner2014medleydb}. There are 1,744 clips of 45 seconds from FMA and 58 full length songs, half of which come from medleyDB and another half from Jamendo.\cite{aljanaki2017developing}
The provided 45 seconds excerpts have been all re-encoded to have the same sampling frequency (i.e, 44100Hz) and have been extracted from random (uniformly distributed) starting point in a given song.\cite{soleymani2016deam}

The music from the FMA was in rock, pop, soul, blues, electronic, classical, hip-hop, international, experimental, folk, jazz, country and pop genres. The music from the MedleyDB dataset in addition had music in world and rap genres, and the music from Jamendo also had reggae music. For 2014 and 2015 data set, music have been manually checked and the files with bad recording quality or those containing speech or noise instead of music have been excluded. For each artist, have been selected no more than 5 songs to be included in the dataset. For medleyDB and Jamendo full-length songs, have been selected songs which had emotional variation in them, using an existing dynamic MER algorithm for filtering and manual final selection\cite{anna2015emotion}.\newline


\subsection{Features from the dataset}

A set of features, extracted by openSMILE\footnote{http://opensmile.sourceforge.net/} for 500ms window is provided with the dataset. 
A list of the given features is reported:

\todo{maybe it's better to provide meaninful names or description}

\begin{itemize}
	\item F0final
	\item voicingFinalUnclipped
	\item jitterLocal
	\item jitterDDP
	\item shimmerLocal
	\item logHNR
	\item audspec\_lengthL1norm
	\item audspecRasta\_lengthL1norm
	\item pcm\_RMSenergy
	\item pcm\_zcr
	\item audSpec\_Rfilt[0 $\rightarrow$ 25]
	\item pcm\_fftMag\_fband250-650, pcm\_fftMag\_fband1000-4000
	\item pcm\_fftMag\_spectralRollOff25.0, pcm\_fftMag\_spectralRollOff50.0, 
	\item pcm\_fftMag\_spectralRollOff75.0, pcm\_fftMag\_spectralRollOff90.0
	\item pcm\_fftMag\_spectralFlux
	\item pcm\_fftMag\_spectralCentroid
	\item pcm\_fftMag\_spectralEntropy
	\item pcm\_fftMag\_spectralVariance
	\item pcm\_fftMag\_spectralSkewness
	\item pcm\_fftMag\_spectralKurtosis
	\item pcm\_fftMag\_spectralSlope
	\item pcm\_fftMag\_psySharpness
	\item pcm\_fftMag\_spectralHarmonicity
	\item pcm\_fftMag\_mfcc[1 $\rightarrow$ 14]
\end{itemize}


\subsection{Extracted features}

Also a set of additional features is extracted  from the 45 seconds song excerpts using \emph{Librosa}\footnote{https://doi.org/10.5281/zenodo.3606573} and 5 different statistical moments are computed for each feature (mean, standard deviation, maximum, minimum and kurtosis):

\begin{itemize}
	\item spectral flatness
	\item tonnetz
	\item chroma\_stft
	\item spectral\_contrast
	\item spectral\_bandwidth
	\item tempogram
	\item harmonic
	\item percussive
	\item tempo
\end{itemize}